"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.lambdaWriteFileImplementation = void 0;
const client_s3_1 = require("@aws-sdk/client-s3");
const lib_storage_1 = require("@aws-sdk/lib-storage");
const mime_types_1 = __importDefault(require("mime-types"));
const content_disposition_header_1 = require("./content-disposition-header");
const get_s3_client_1 = require("./get-s3-client");
// Files larger than 100MB will use multipart upload
const MULTIPART_THRESHOLD = 100 * 1024 * 1024; // 5MB in bytes
const tryLambdaWriteFile = async ({ bucketName, key, body, region, privacy, expectedBucketOwner, downloadBehavior, customCredentials, forcePathStyle, storageClass, requestHandler, }) => {
    const client = (0, get_s3_client_1.getS3Client)({
        region,
        customCredentials: customCredentials,
        forcePathStyle,
        requestHandler,
    });
    const params = {
        Bucket: bucketName,
        Key: key,
        Body: body,
        ACL: privacy === 'no-acl'
            ? undefined
            : (privacy === 'private'
                ? 'private'
                : 'public-read'),
        ExpectedBucketOwner: customCredentials
            ? undefined
            : (expectedBucketOwner !== null && expectedBucketOwner !== void 0 ? expectedBucketOwner : undefined),
        ContentType: mime_types_1.default.lookup(key) || 'application/octet-stream',
        ContentDisposition: (0, content_disposition_header_1.getContentDispositionHeader)(downloadBehavior),
        StorageClass: storageClass !== null && storageClass !== void 0 ? storageClass : undefined,
    };
    // Determine file size
    const size = body instanceof Buffer || body instanceof Uint8Array
        ? body.length
        : body instanceof Blob
            ? body.size
            : typeof body === 'string'
                ? Buffer.from(body).length
                : null;
    // Use multipart upload for large files or streams (where we can't determine size)
    if (size === null || size > MULTIPART_THRESHOLD) {
        const upload = new lib_storage_1.Upload({
            client,
            params,
            queueSize: 4, // number of concurrent uploads
            partSize: 5 * 1024 * 1024, // chunk size of 5MB
        });
        await upload.done();
    }
    else {
        // Use regular PutObject for small files
        await client.send(new client_s3_1.PutObjectCommand(params));
    }
};
const lambdaWriteFileImplementation = async (params) => {
    var _a;
    const remainingRetries = (_a = params.retries) !== null && _a !== void 0 ? _a : 2;
    try {
        await tryLambdaWriteFile(params);
    }
    catch (err) {
        if (remainingRetries === 0) {
            throw err;
        }
        const backoff = 2 ** (2 - remainingRetries) * 2000;
        await new Promise((resolve) => {
            setTimeout(resolve, backoff);
        });
        console.warn('Failed to write file to storage:');
        console.warn(err);
        console.warn(`Retrying (${remainingRetries} retries remaining)...`);
        return (0, exports.lambdaWriteFileImplementation)({
            ...params,
            retries: remainingRetries - 1,
        });
    }
};
exports.lambdaWriteFileImplementation = lambdaWriteFileImplementation;
